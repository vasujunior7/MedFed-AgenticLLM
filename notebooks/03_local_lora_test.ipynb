{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e121e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  6 06:13:58 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             54W /  300W |   31166MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             55W /  300W |     828MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             41W /  300W |       6MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  |   00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             41W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           On  |   00000000:85:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             39W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           On  |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             40W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           On  |   00000000:89:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             41W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           On  |   00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             57W /  300W |    3556MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b34fbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flwr\n",
      "  Downloading flwr-1.25.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting click<8.2.0 (from flwr)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting cryptography<45.0.0,>=44.0.1 (from flwr)\n",
      "  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /root/miniconda/lib/python3.13/site-packages (from flwr) (1.73.1)\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.70.0 (from flwr)\n",
      "  Downloading grpcio_health_checking-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
      "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /root/miniconda/lib/python3.13/site-packages (from flwr) (2.1.2)\n",
      "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: protobuf<7.0.0,>=5.28.0 in /root/miniconda/lib/python3.13/site-packages (from flwr) (5.29.5)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /root/miniconda/lib/python3.13/site-packages (from flwr) (3.23.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /root/miniconda/lib/python3.13/site-packages (from flwr) (6.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /root/miniconda/lib/python3.13/site-packages (from flwr) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /root/miniconda/lib/python3.13/site-packages (from flwr) (13.9.4)\n",
      "Collecting tomli<3.0.0,>=2.0.1 (from flwr)\n",
      "  Downloading tomli-2.3.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
      "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typer<0.21.0,>=0.12.5 in /root/miniconda/lib/python3.13/site-packages (from flwr) (0.19.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/miniconda/lib/python3.13/site-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
      "Collecting protobuf<7.0.0,>=5.28.0 (from flwr)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting grpcio<2.0.0,>=1.70.0 (from flwr)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /root/miniconda/lib/python3.13/site-packages (from grpcio<2.0.0,>=1.70.0->flwr) (4.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->flwr) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->flwr) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->flwr) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.10.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda/lib/python3.13/site-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda/lib/python3.13/site-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/miniconda/lib/python3.13/site-packages (from typer<0.21.0,>=0.12.5->flwr) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /root/miniconda/lib/python3.13/site-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.21)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.0)\n",
      "Downloading flwr-1.25.0-py3-none-any.whl (727 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m727.1/727.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading grpcio_health_checking-1.76.0-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading tomli-2.3.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (248 kB)\n",
      "Downloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
      "Installing collected packages: tomli-w, tomli, protobuf, pathspec, iterators, grpcio, click, grpcio-health-checking, cryptography, flwr\n",
      "\u001b[2K  Attempting uninstall: protobuf\n",
      "\u001b[2K    Found existing installation: protobuf 5.29.5\n",
      "\u001b[2K    Uninstalling protobuf-5.29.5:\n",
      "\u001b[2K      Successfully uninstalled protobuf-5.29.5\n",
      "\u001b[2K  Attempting uninstall: grpcio\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: grpcio 1.73.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling grpcio-1.73.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.73.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: click[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: click 8.2.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling click-8.2.1:[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled click-8.2.1mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: cryptography\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: cryptography 45.0.3â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling cryptography-45.0.3:â•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled cryptography-45.0.3â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [click]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/10\u001b[0m [flwr]2m 9/10\u001b[0m [flwr]ography]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.2 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed click-8.1.8 cryptography-44.0.3 flwr-1.25.0 grpcio-1.76.0 grpcio-health-checking-1.76.0 iterators-0.0.2 pathspec-0.12.1 protobuf-6.33.2 tomli-2.3.0 tomli-w-1.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install flwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0456ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:15:36.540086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ PyTorch version: 2.9.0+cu128\n",
      "ğŸ® CUDA available: True\n",
      "ğŸ¯ GPU: Tesla V100-SXM2-32GB\n",
      "ğŸ’¾ Initial VRAM: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"ğŸ”§ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ® CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ¯ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ Initial VRAM: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57587d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  6 06:15:40 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             54W /  300W |   31166MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             55W /  300W |     828MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             41W /  300W |       6MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  |   00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             41W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           On  |   00000000:85:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             39W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           On  |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             40W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           On  |   00000000:89:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             41W /  300W |       8MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           On  |   00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             57W /  300W |    3556MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bb6e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Loaded 4520 samples from Hospital A\n",
      "\n",
      "ğŸ“ Sample entry:\n",
      "  - Text length: 1276 chars\n",
      "  - Token count: 297\n",
      "  - Preview: User: What is the primary property lost by alveolar Type II Pneumocytes in culture Assistant: Surfactant production is important in maintaining alveolar function both in vivo and in vitro, but surfact...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"../data/processed/hospital_A/dataset.jsonl\"\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "train_data = load_jsonl(dataset_path)\n",
    "print(f\"\\nâœ… Loaded {len(train_data)} samples from Hospital A\")\n",
    "print(f\"\\nğŸ“ Sample entry:\")\n",
    "sample = train_data[0]\n",
    "print(f\"  - Text length: {len(sample['text'])} chars\")\n",
    "print(f\"  - Token count: {sample['token_length']}\")\n",
    "print(f\"  - Preview: {sample['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3383287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading tokenizer...\n",
      "ğŸ”„ Loading model with 4-bit quantization on GPU 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b2645a08cd48879182b9b8037a7e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ VRAM after model load: 0.80 GB\n",
      "âœ… Model loaded successfully on GPU 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"ğŸ”„ Loading model with 4-bit quantization on GPU 3...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Prepare for k-bit training (gradient checkpointing)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ’¾ VRAM after model load: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(\"âœ… Model loaded successfully on GPU 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d410c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LoRA adapters applied\n",
      "ğŸ“Š Trainable params: 3,407,872 (0.09%)\n",
      "ğŸ“Š Total params: 3,755,479,040\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                           # LoRA rank\n",
    "    lora_alpha=16,                 # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Attention modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"âœ… LoRA adapters applied\")\n",
    "print(f\"ğŸ“Š Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"ğŸ“Š Total params: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small subset for testing\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"AI-MO/ai-medical-dataset\")\n",
    "\n",
    "# Use small subset for quick test\n",
    "train_data = dataset['train'].select(range(100))\n",
    "print(f\"âœ“ Loaded {len(train_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "print(\"Preprocessing...\")\n",
    "processed_data = preprocess_dataset(train_data, tokenizer, max_length=512)\n",
    "print(\"âœ“ Dataset preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dae72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer = train_local(\n",
    "    model=model,\n",
    "    train_dataset=processed_data,\n",
    "    output_dir=\"../output-models/test-lora\"\n",
    ")\n",
    "print(\"âœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f966f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "from src.model.inference import generate_response\n",
    "\n",
    "test_question = \"What causes type 2 diabetes?\"\n",
    "response = generate_response(model, tokenizer, test_question)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage\n",
    "print(f\"\\nAllocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368e500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb6a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
