#!/usr/bin/env python3
"""
Minimal Testing Suite - Milestone 9
Tests core functionality: dataset loading, agent weights, federated aggregation.

Usage:
    python test_minimal.py
    pytest test_minimal.py -v
"""

import os
import sys
import json
import numpy as np
from pathlib import Path

# Add src to path
sys.path.append(os.path.dirname(__file__))
from src.agent.coordinator import AgenticAggregator


def test_dataset_loading():
    """Test 1: Verify dataset files exist and can be loaded."""
    print("\n" + "="*70)
    print("TEST 1: Dataset Loading")
    print("="*70)
    
    hospitals = ['hospital_A', 'hospital_B', 'hospital_C']
    results = {}
    
    for hospital in hospitals:
        dataset_path = f"data/processed/{hospital}/dataset.jsonl"
        
        # Check file exists
        assert os.path.exists(dataset_path), f"Dataset not found: {dataset_path}"
        
        # Load and validate
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        # Validate non-empty
        assert len(data) > 0, f"{hospital} has no data"
        
        # Validate structure
        sample = data[0]
        assert 'text' in sample, f"{hospital} missing 'text' field"
        assert 'token_length' in sample, f"{hospital} missing 'token_length' field"
        assert isinstance(sample['text'], str), f"{hospital} 'text' is not string"
        assert isinstance(sample['token_length'], int), f"{hospital} 'token_length' is not int"
        
        results[hospital] = len(data)
        print(f"âœ… {hospital}: {len(data):,} samples loaded")
    
    # Validate no overlaps (verify federated split)
    print("\nğŸ“Š Checking for data overlap...")
    indices = {}
    for hospital in hospitals:
        dataset_path = f"data/processed/{hospital}/dataset.jsonl"
        with open(dataset_path, 'r') as f:
            data = [json.loads(line) for line in f]
        indices[hospital] = set(d.get('index', d.get('id', i)) for i, d in enumerate(data))
    
    overlap_AB = len(indices['hospital_A'] & indices['hospital_B'])
    overlap_AC = len(indices['hospital_A'] & indices['hospital_C'])
    overlap_BC = len(indices['hospital_B'] & indices['hospital_C'])
    
    assert overlap_AB == 0, f"Overlap between A and B: {overlap_AB} samples"
    assert overlap_AC == 0, f"Overlap between A and C: {overlap_AC} samples"
    assert overlap_BC == 0, f"Overlap between B and C: {overlap_BC} samples"
    
    print("âœ… No overlaps detected - federated split valid")
    print(f"âœ… Total samples: {sum(results.values()):,}")
    
    return results


def test_agent_weight_validity():
    """Test 2: Verify agent weight computation is valid."""
    print("\n" + "="*70)
    print("TEST 2: Agent Weight Validity")
    print("="*70)
    
    aggregator = AgenticAggregator(loss_weight=0.6, variance_weight=0.4)
    
    # Test Case 1: Equal clients
    print("\nğŸ“Š Test Case 1: Equal performance clients")
    client_metrics = [
        {'hospital': 'A', 'initial_loss': 10.0, 'final_loss': 2.0, 'num_samples': 1000},
        {'hospital': 'B', 'initial_loss': 10.0, 'final_loss': 2.0, 'num_samples': 1000},
        {'hospital': 'C', 'initial_loss': 10.0, 'final_loss': 2.0, 'num_samples': 1000},
    ]
    
    weights, analysis = aggregator.compute_aggregation_weights(
        client_metrics, 
        sample_counts=[1000, 1000, 1000]
    )
    
    # Validate weights
    assert len(weights) == 3, "Should return 3 weights"
    assert all(w >= 0 for w in weights), "All weights should be non-negative"
    assert abs(sum(weights) - 1.0) < 1e-6, f"Weights should sum to 1.0, got {sum(weights)}"
    
    print(f"   Weights: {[f'{w:.3f}' for w in weights]}")
    print(f"   Sum: {sum(weights):.6f}")
    print("   âœ… Weights are valid probabilities")
    
    # Test Case 2: Diverse performance
    print("\nğŸ“Š Test Case 2: Diverse performance clients")
    client_metrics = [
        {'hospital': 'A', 'initial_loss': 10.0, 'final_loss': 1.0, 'num_samples': 1000},  # Best
        {'hospital': 'B', 'initial_loss': 10.0, 'final_loss': 5.0, 'num_samples': 1000},  # Medium
        {'hospital': 'C', 'initial_loss': 10.0, 'final_loss': 9.0, 'num_samples': 1000},  # Worst
    ]
    
    weights, analysis = aggregator.compute_aggregation_weights(
        client_metrics,
        sample_counts=[1000, 1000, 1000]
    )
    
    # Best performer should have highest weight
    best_idx = np.argmin([m['final_loss'] for m in client_metrics])
    worst_idx = np.argmax([m['final_loss'] for m in client_metrics])
    
    assert weights[best_idx] > weights[worst_idx], "Best performer should have higher weight"
    assert abs(sum(weights) - 1.0) < 1e-6, "Weights should sum to 1.0"
    
    print(f"   Best (A): {weights[0]:.3f}")
    print(f"   Medium (B): {weights[1]:.3f}")
    print(f"   Worst (C): {weights[2]:.3f}")
    print("   âœ… Best performer has highest weight")
    
    # Test Case 3: Different sample sizes
    print("\nğŸ“Š Test Case 3: Different sample sizes")
    weights, analysis = aggregator.compute_aggregation_weights(
        client_metrics,
        sample_counts=[5000, 2000, 1000]  # A has 5x more data than C
    )
    
    assert abs(sum(weights) - 1.0) < 1e-6, "Weights should sum to 1.0"
    assert all(w >= 0 for w in weights), "All weights should be non-negative"
    
    print(f"   Weights with sample size consideration:")
    print(f"   A (5000 samples): {weights[0]:.3f}")
    print(f"   B (2000 samples): {weights[1]:.3f}")
    print(f"   C (1000 samples): {weights[2]:.3f}")
    print("   âœ… Sample size affects aggregation weights")
    
    return weights, analysis


def test_federated_aggregation():
    """Test 3: Verify federated aggregation execution."""
    print("\n" + "="*70)
    print("TEST 3: Federated Aggregation Execution")
    print("="*70)
    
    # Simulate client parameters (LoRA weights)
    print("\nğŸ“Š Simulating client LoRA parameters...")
    np.random.seed(42)
    
    num_params = 10  # Simplified (real LoRA has 3.4M params)
    client_params = [
        [np.random.randn(num_params) for _ in range(3)],  # Client A: 3 layers
        [np.random.randn(num_params) for _ in range(3)],  # Client B: 3 layers
        [np.random.randn(num_params) for _ in range(3)],  # Client C: 3 layers
    ]
    
    print(f"   Client A: {len(client_params[0])} layers, {num_params} params each")
    print(f"   Client B: {len(client_params[1])} layers, {num_params} params each")
    print(f"   Client C: {len(client_params[2])} layers, {num_params} params each")
    
    # Compute aggregation weights
    aggregator = AgenticAggregator(loss_weight=0.6, variance_weight=0.4)
    client_metrics = [
        {'hospital': 'A', 'initial_loss': 0.5, 'final_loss': 0.1, 'num_samples': 4520},
        {'hospital': 'B', 'initial_loss': 0.5, 'final_loss': 0.05, 'num_samples': 2521},  # Best
        {'hospital': 'C', 'initial_loss': 0.5, 'final_loss': 0.2, 'num_samples': 2959},
    ]
    
    weights, analysis = aggregator.compute_aggregation_weights(
        client_metrics,
        sample_counts=[4520, 2521, 2959]
    )
    
    print(f"\nâš–ï¸  Agent weights: {[f'{w:.3f}' for w in weights]}")
    
    # Perform weighted aggregation
    print("\nğŸ”„ Performing weighted aggregation...")
    aggregated_params = []
    
    for layer_idx in range(len(client_params[0])):
        # Aggregate this layer across all clients
        layer_params = [client_params[i][layer_idx] for i in range(3)]
        aggregated_layer = np.average(layer_params, axis=0, weights=weights)
        aggregated_params.append(aggregated_layer)
    
    print(f"âœ… Aggregated {len(aggregated_params)} layers")
    
    # Validate aggregation
    print("\nğŸ“Š Validating aggregation...")
    
    # Check shapes match
    for i, (agg, orig) in enumerate(zip(aggregated_params, client_params[0])):
        assert agg.shape == orig.shape, f"Layer {i} shape mismatch"
    
    print("   âœ… Shapes preserved")
    
    # Check aggregated values are within range of client values
    for layer_idx, agg_layer in enumerate(aggregated_params):
        client_mins = [client_params[i][layer_idx].min() for i in range(3)]
        client_maxs = [client_params[i][layer_idx].max() for i in range(3)]
        
        overall_min = min(client_mins)
        overall_max = max(client_maxs)
        
        assert agg_layer.min() >= overall_min - 1e-6, f"Layer {layer_idx} below min"
        assert agg_layer.max() <= overall_max + 1e-6, f"Layer {layer_idx} above max"
    
    print("   âœ… Aggregated values within valid range")
    
    # Verify weighted average property
    # For first parameter, manually check weighted average
    layer_0_manual = sum(weights[i] * client_params[i][0] for i in range(3))
    assert np.allclose(aggregated_params[0], layer_0_manual), "Weighted average incorrect"
    
    print("   âœ… Weighted averaging correct")
    
    # Check that different weights produce different results
    uniform_weights = [1/3, 1/3, 1/3]
    uniform_agg = np.average([client_params[i][0] for i in range(3)], axis=0, weights=uniform_weights)
    
    # They should be different (unless by chance weights were already uniform)
    if not np.allclose(weights, uniform_weights):
        assert not np.allclose(aggregated_params[0], uniform_agg), "Agent weights should affect result"
        print("   âœ… Agent weights affect aggregation (non-uniform)")
    
    return aggregated_params


def test_training_artifacts():
    """Test 4: Verify training artifacts exist."""
    print("\n" + "="*70)
    print("TEST 4: Training Artifacts")
    print("="*70)
    
    # Check federated models exist
    print("\nğŸ“‚ Checking federated model artifacts...")
    hospitals = ['hospital_A', 'hospital_B', 'hospital_C']
    
    for hospital in hospitals:
        model_dir = f"output-models/federated/{hospital}/final"
        
        if os.path.exists(model_dir):
            # Check for LoRA adapter files
            adapter_file = os.path.join(model_dir, "adapter_model.safetensors")
            config_file = os.path.join(model_dir, "adapter_config.json")
            
            assert os.path.exists(adapter_file), f"{hospital} missing adapter model"
            assert os.path.exists(config_file), f"{hospital} missing adapter config"
            
            # Check adapter size
            adapter_size = os.path.getsize(adapter_file) / (1024**2)  # MB
            assert adapter_size > 0, f"{hospital} adapter file is empty"
            
            print(f"âœ… {hospital}: adapter_model.safetensors ({adapter_size:.2f} MB)")
        else:
            print(f"âš ï¸  {hospital} model directory not found (may not be trained yet)")
    
    # Check training metrics
    print("\nğŸ“Š Checking training metrics...")
    metrics_file = "output-models/federated/metrics/training_history.json"
    
    if os.path.exists(metrics_file):
        with open(metrics_file, 'r') as f:
            history = json.load(f)
        
        assert 'rounds' in history, "Missing 'rounds' in training history"
        assert 'global_losses' in history, "Missing 'global_losses' in training history"
        assert len(history['rounds']) > 0, "No training rounds recorded"
        
        print(f"âœ… Training history: {len(history['rounds'])} rounds")
        print(f"   Global losses: {history['global_losses']}")
    else:
        print(f"âš ï¸  Training metrics not found (may not be trained yet)")
    
    return True


def test_safety_guardrails():
    """Test 5: Verify safety guardrails functionality."""
    print("\n" + "="*70)
    print("TEST 5: Safety Guardrails")
    print("="*70)
    
    from src.safety.guardrails import MedicalGuardrails
    
    guardrails = MedicalGuardrails()
    
    # Test 1: Safe response
    print("\nğŸ“Š Test Case 1: Safe response")
    safe_response = "Diabetes symptoms include increased thirst. Please consult your doctor."
    result = guardrails.check_response(safe_response)
    
    assert result['is_safe'] == True, "Safe response should pass"
    print("   âœ… Safe response detected correctly")
    
    # Test 2: Response with disclaimer
    print("\nğŸ“Š Test Case 2: Disclaimer check")
    assert guardrails._has_disclaimer(safe_response), "Should detect disclaimer keywords"
    print("   âœ… Disclaimer detection works")
    
    # Test 3: Add disclaimer
    print("\nğŸ“Š Test Case 3: Add disclaimer")
    response_without = "Diabetes is a chronic condition."
    response_with = guardrails.add_disclaimer(response_without)
    
    assert len(response_with) > len(response_without), "Disclaimer should be added"
    assert "âš ï¸" in response_with, "Should contain warning emoji"
    assert "healthcare professional" in response_with.lower(), "Should mention healthcare professional"
    print("   âœ… Disclaimer addition works")
    
    return True


def run_all_tests():
    """Run all minimal tests."""
    print("\n" + "="*70)
    print("ğŸ§ª MINIMAL TESTING SUITE - MILESTONE 9")
    print("="*70)
    print("\nTesting core functionality:")
    print("  1. Dataset loading")
    print("  2. Agent weight validity")
    print("  3. Federated aggregation execution")
    print("  4. Training artifacts")
    print("  5. Safety guardrails")
    
    results = {}
    
    try:
        # Test 1: Dataset loading
        results['dataset_loading'] = test_dataset_loading()
        
        # Test 2: Agent weights
        results['agent_weights'] = test_agent_weight_validity()
        
        # Test 3: Federated aggregation
        results['federated_aggregation'] = test_federated_aggregation()
        
        # Test 4: Training artifacts
        results['training_artifacts'] = test_training_artifacts()
        
        # Test 5: Safety guardrails
        results['safety_guardrails'] = test_safety_guardrails()
        
        # Summary
        print("\n" + "="*70)
        print("âœ… ALL TESTS PASSED - MILESTONE 9 COMPLETE!")
        print("="*70)
        
        print("\nğŸ“Š Test Summary:")
        print("   âœ… Dataset loading: Valid federated split")
        print("   âœ… Agent weights: Valid probability distribution")
        print("   âœ… Federated aggregation: Weighted averaging correct")
        print("   âœ… Training artifacts: Models and metrics present")
        print("   âœ… Safety guardrails: Disclaimer system functional")
        
        print("\nğŸ¯ Confidence Level: HIGH")
        print("   - Core functionality verified")
        print("   - Federated learning pipeline operational")
        print("   - Safety measures in place")
        
        return True
        
    except AssertionError as e:
        print(f"\nâŒ TEST FAILED: {e}")
        return False
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
